{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一节所构造的线性神经网络，各计算单元（模型，损失，优化方法）都是基于原理直接编写的。但面对更加复杂的网络时，这样从底层编写显然缺乏效率。利用torch的高级API可以大大加快编写的速度。因此第二节内容中，我们使用torch自带的API编写线性神经网络，并用于之前从底层白那些的神经网络中，各模块的实现进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch as pt\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as Data\n",
    "from torch.nn import init\n",
    "from linear_regression import *\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先定义模型。在torch的框架中，已经定义了大部分常用的神经网络模型模板，在实际使用中，我们可以“继承”torch的模板，在此基础上编写我们需要的神经网络结构。继承的模板中必须要包含初始化模型参数（__init__()），以及模板的输出（forward()）函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_network(pt.nn.Module):\n",
    "    def __init__(self,data_dim):\n",
    "        super(linear_network,self).__init__()\n",
    "        self.linear = pt.nn.Linear(data_dim,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在torch的框架下，虽然引入了大量的API加快我们构建网络的速度，但诸如模型参数初始化，模型训练等内容还需要单独定义函数。相对于从底层编写，torch自带的API能简化我们的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression_torch(linear_regression):\n",
    "    def __init__(self,batch_size=None,learning_rate=None,epoch=None,data_dim=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.net=linear_network(data_dim=data_dim)\n",
    "\n",
    "    def generate_train_data(self,x,y):\n",
    "        dataset = Data.TensorDataset(x,y)\n",
    "        data_iter = Data.DataLoader(dataset,self.batch_size,shuffle=True)\n",
    "        return data_iter\n",
    "\n",
    "    def model_init(self):\n",
    "        init.normal_(self.net.linear.weight,mean=1,std=0.01)\n",
    "        init.constant_(self.net.linear.bias,val=0)\n",
    "        self.loss_function=pt.nn.MSELoss()\n",
    "        self.optimizer=optim.SGD(self.net.parameters(),lr=self.lr)\n",
    "\n",
    "    def train_linear_model(self,x,y):\n",
    "        for i in tqdm(range(self.epoch)):\n",
    "            for X,Y in self.generate_train_data(x,y):\n",
    "                loss=self.loss_function(self.net(X),Y.view(-1,1))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        print('loss=',loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    LinearNet=linear_regression_torch(batch_size=10,learning_rate=0.001,epoch=200,data_dim=3)\n",
    "    x,y=LinearNet.generate_test_data(3)\n",
    "    LinearNet.model_init()\n",
    "    LinearNet.train_linear_model(x,y)\n",
    "    print(LinearNet.net.linear.weight)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d024a7ccc8c9584276d35bfad2121bc41d4ebc6d5b0f18b4a6f2ce22e14c0c40"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
